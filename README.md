# Enhanced IMDb Scraper

A comprehensive movie data extraction and management system with advanced scraping capabilities, media downloading, and rich API features.

## Features

- ğŸ¬ **Complete Movie Data**: Extract detailed information including cast, crew, media, and reviews
- ğŸ“¸ **Media Management**: Download and store movie posters, stills, and trailers
- ğŸ” **Advanced Search**: Date-based filtering with comprehensive search capabilities
- ğŸ—„ï¸ **Rich Database**: Normalized schema with proper indexing and relationships
- ğŸš€ **Modern API**: FastAPI with async support and comprehensive documentation
- ğŸ“Š **Analytics Dashboard**: Rich visualizations and data insights
- ğŸ³ **Docker Ready**: Complete containerization with development and production configs

## Quick Start

1. **Clone and setup:**
   ```bash
   git clone <repository>
   cd enhanced-imdb-scraper
   chmod +x scripts/setup.sh
   ./scripts/setup.sh
   ```

2. **Start development environment:**
   ```bash
   docker-compose up
   ```

3. **Access the application:**
   - Frontend: http://localhost:8501
   - API Docs: http://localhost:8000/api/docs
   - MinIO Console: http://localhost:9001

## Project Structure

```
enhanced-imdb-scraper/
â”œâ”€â”€ backend/               # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/          # API endpoints
â”‚   â”‚   â”œâ”€â”€ core/         # Core configuration
â”‚   â”‚   â”œâ”€â”€ models/       # Database and Pydantic models
â”‚   â”‚   â”œâ”€â”€ services/     # Business logic
â”‚   â”‚   â””â”€â”€ workers/      # Background tasks
â”‚   â”œâ”€â”€ alembic/          # Database migrations
â”‚   â””â”€â”€ tests/            # Test suite
â”œâ”€â”€ frontend/             # Streamlit frontend
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/   # Reusable UI components
â”‚       â”œâ”€â”€ pages/        # Application pages
â”‚       â””â”€â”€ services/     # API integration
â”œâ”€â”€ data/                 # Data storage
â”‚   â”œâ”€â”€ downloads/        # Downloaded media
â”‚   â”œâ”€â”€ exports/          # Exported data
â”‚   â””â”€â”€ backups/          # Database backups
â””â”€â”€ docs/                 # Documentation
```

## API Endpoints

### Movies
- `GET /api/v1/movies/` - List movies with pagination
- `GET /api/v1/movies/{id}` - Get movie details
- `POST /api/v1/movies/` - Create movie
- `PUT /api/v1/movies/{id}` - Update movie
- `DELETE /api/v1/movies/{id}` - Delete movie

### Search
- `GET /api/v1/search/movies` - Search movies
- `GET /api/v1/search/people` - Search people
- `GET /api/v1/search/advanced` - Advanced search

### Scraping
- `POST /api/v1/scraping/start` - Start scraping job
- `GET /api/v1/scraping/status/{job_id}` - Get job status
- `GET /api/v1/scraping/jobs` - List all jobs

### Media
- `GET /api/v1/media/images/{movie_id}` - Get movie images
- `GET /api/v1/media/videos/{movie_id}` - Get movie videos
- `POST /api/v1/media/download` - Download media

## Configuration

Copy `.env.example` to `.env` and configure:

```env
# Database
DATABASE_URL=postgresql+asyncpg://user:pass@localhost:5432/imdb_scraper

# External APIs (optional but recommended)
IPSTACK_API_KEY=your_ipstack_key
TMDB_API_KEY=your_tmdb_key

# Proxy settings (for scraping)
PROXY_HOST=your.proxy.com
PROXY_PORT=8080
PROXY_USERNAME=username
PROXY_PASSWORD=password
```

## Development

1. **Backend development:**
   ```bash
   cd backend
   pip install -r requirements.txt
   uvicorn app.main:app --reload
   ```

2. **Frontend development:**
   ```bash
   cd frontend
   pip install -r requirements.txt
   streamlit run src/app.py
   ```

3. **Database migrations:**
   ```bash
   cd backend
   alembic revision --autogenerate -m "Description"
   alembic upgrade head
   ```

4. **Run tests:**
   ```bash
   cd backend
   pytest
   ```

## Deployment

### Production with Docker
```bash
chmod +x scripts/deploy.sh
./scripts/deploy.sh
```

### Manual deployment
1. Set production environment variables
2. Build Docker images
3. Deploy with docker-compose
4. Run database migrations
5. Configure reverse proxy (nginx)

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## License

MIT License - see LICENSE file for details.
# IMBD-Scraper
